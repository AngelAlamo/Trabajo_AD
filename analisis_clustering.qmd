---
title: "Análisis con clustering"
format: html
editor: visual
author: 
  - name: Ángel Álamo
  - name: Juanjo Doblas
  - name: Óscar Vanrell 
execute:
  echo: false
---

En este documento vamos a realizar un análisis con Clustering de las variables cuantitativas de nuestro dataset Spotify 2023. Primero, cargamos las librerías que utilizaremos y el tibble que contenga las variables cuantitativas.

```{r librerias, warning=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library("factoextra")
library("ggfortify")
library(cluster)
```

```{r dataset var cuant, warning=FALSE}
spotify = read_csv(file = "spotify-2023.csv", show_col_types = FALSE)

breaks_bpm = c("60", "100", "120", "210")

# Cambiamos el tipo de los valores
spotify2 <- spotify %>% 
  mutate(streams = as.numeric(streams),
         released_month = as.ordered(released_month),
         released_day = as.ordered(released_day),
         key = factor(key, levels = c("C", "C#", "D", "D#", "E", "F", "G", "G#", "A", "A#", "B"))) %>%
  mutate(across(where(is.character), as.factor)) %>% # chr a factor
  mutate(artist_count = as.factor(artist_count)) %>% 
  
  # Creamos la variable colaboración
  mutate("collaboration" = case_when(
    artist_count == 1 ~ "solo",
    artist_count %in% 2:10 ~ "collaboration", 
    NA ~ NA)) %>%
  
  # Creamos y añadimos la variable reproducciones por artista y la posicionamos antes de el dia de lanzamiento
  group_by(`artist(s)_name`) %>%
  #mutate("songs_per_artist" = n()) %>% 
  mutate("artist_streams" = sum(streams)) %>%
  ungroup() %>%
  #relocate(songs_per_artist, .before = released_year )
  
  # Creamos las variables "rango_bpm" ; "tempo"
  mutate(rango_bpm = cut(bpm, breaks = breaks_bpm, include.lowest = TRUE),
         tempo = factor(case_when(
           between(bpm, 60, 100) ~ "Lenta",
           between(bpm, 101, 120) ~ "Normal",
           between(bpm, 121, 210) ~ "Rápida"
         ))) %>% 
  relocate(rango_bpm, .before = key) %>% 
  relocate(tempo, .after = rango_bpm) %>% 
  
  # Renombramos variables
  dplyr::rename("dance%" = `danceability_%`,
         "valence%" = `valence_%`,
        "energy%" = `energy_%`,
         "acoustic%" = `acousticness_%`,
         "instrumental%" = `instrumentalness_%`,
         "live%" = `liveness_%`,
         "speech%" = `speechiness_%`,
         artist = `artist(s)_name`) %>% 
  
  # Creamos la variable epoca
  mutate(epoca = case_when(
  between(released_year, 1930, 1999) ~ "Epoca_1",
  between(released_year, 2000, 2015) ~ "Epoca_2",
  between(released_year, 2016, 2021) ~ "Epoca_3",
  between(released_year, 2022, 2023) ~ "Epoca_4",
  )) %>% 
  relocate(epoca, .after = released_year) %>% 
  mutate(epoca = ordered(epoca, labels = c("Epoca_1", "Epoca_2", "Epoca_3", "Epoca_4"))) %>% 
  
  # Añadimos la variable estación de lanzamiento
  mutate("released_season" = case_when(
    released_month %in% c(12, 1, 2) ~ "winter",
    released_month %in% 3:5 ~ "spring",
    released_month %in% 6:8 ~ "summer",
    released_month %in% 9:11 ~ "autumn",
    NA ~ NA)) %>% 
  
  # Necesario para definir la variable época
  mutate(released_year = as.ordered(released_year)) %>% 

  # Eliminamos ciertas variables
  dplyr::select(!contains("charts")) %>% 
  
  # Recolocamos las variables
  relocate(streams, .after = artist_count) %>% 
  relocate(released_day, .before = released_year) %>% 
  relocate(released_month, .before = released_year) %>% 
  relocate(artist, .before = track_name) %>%
  relocate(released_season, .before = released_year) %>%
  relocate(artist_streams, .before = released_day) %>%
  relocate(collaboration, .before = streams)

# Orden personalizado de las estaciones del año
orden_estaciones <- c("spring", "summer", "autumn", "winter")

# Cambia el orden de las estaciones del año
spotify2$released_season <- factor(spotify2$released_season,
                                   levels = orden_estaciones)

spotify_cuant = spotify2 %>% 
  dplyr::select(is.double) %>% 
  na.omit()

spotify_cuant %>% 
  glimpse
```

Primero haremos un Análisis de Componentes Principales sobre estos datos para poder tener una representación de estos puntos. Esta representación se hará únicamente con las variables porcentuales, por dos razones: estas variables están en la misma unidad de mesura y queremos tener en cuenta la variabilidad de estas variables, por lo tanto considerando solo las porcentuales no es necesario tipificar (únicamente centrar); en este caso, al ser estas observaciones las canciones más escuchadas, tiene más sentido representar estas canciones según las propiedades de estas.


```{r}
spotify_perc = spotify_cuant %>% 
  select(contains("%"))

spotify_perc
```

Así pues, nuestra tabla de datos es la siguiente (con los datos ya centrados):

```{r}

# Número de filas
n = nrow(spotify_perc)

# Matriz de centrado
Hn = diag(n) - 1/n

# Datos centrados
sp_perc_cen = as_tibble(Hn %*% as.matrix(spotify_perc))

sp_perc_cen %>% 
  glimpse
```

El análisis de la matriz de covarianzas y la matriz de correlación se hizo anteriormente, pasamos a calcular las componentes principales con la función `r prcomp`:

```{r}

sp_perc_cen_acp = prcomp(sp_perc_cen, scale = FALSE)


```

Veamos cuántas dimensiones debemos considerar para la representación, para ello veamos la variabilidad que explica cada valor propio junto a la variabilidad acumulativa.

```{r}
vaps_perc = get_eigenvalue(sp_perc_cen_acp)
vaps_perc
```

Con $2$ dimensiones, el porcentaje que explican de varianza es de aproximadamente un $68.94%$, en cambio, si tomamos $3$ dimensiones, este aumenta, explicando aproximadamente un $79.06%$. Tenemos dos opciones: consideramos $2$ dimensiones, para obtener una representación más sencilla de los datos pero con una explicación de la varianza menos acertada y por lo tanto menos precisa; consideramos $3$ dimensiones, así la representación es más ajustada y consigue explicar más varianza, pero con una representación más tediosa. Usemos el diagrama de valores propios para determinar el número de componentes principales.

```{r}
fviz_eig(sp_perc_cen_acp, addlabels = TRUE, ylim = c(0,100))

```

Si consideramos dos componentes principales, la representación de los datos es la siguiente:


```{r}
autoplot(sp_perc_cen_acp, data = sp_perc_cen,
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
```


Lo dividiremos en tres partes, donde realizaremos tres técnicas aprendidas en clase: el método de *k-means*, el método *k-medoids* y el método de clustering jerárquico aglomerativo.


Debido a la gran cantidad de observaciones que tenemos en el dataset y para una mayor facilidad a la hora de construir los $\textit{clusters}$, consideraremos las $50$ primeras canciones (observaciones):

```{r}
# 50 primeras observaciones

spotify_perc_50 = spotify_perc %>% 
  slice_head(n = 50)

spotify_perc_50 %>% 
  glimpse

```



## Método k-means







## Método k-medoids

Recordemos que para aplicar el método *k-medoids*, como en el método *k-means*, se necesitan $k$ elementos para iniciar el algoritmo. Primero, necesitamos encontrar la $k$ óptima, para esto, utilizaremos la función `r fviz_nbclust()`, con la distancia euclidea y la de Manhattan.


```{r}
# Euclidea
fviz_nbclust(x = spotify_perc_50, FUNcluster = pam, method = "wss", diss = dist(spotify_perc_50, method = "euclidean")) +
  labs(title = "Número de clusters óptimos", subtitle = "Distancia Euclidea", x = "Número de clusters", y = "Suma total de los cuadrados (wss) ") +
  theme(plot.title = element_text(size = 20), plot.subtitle = element_text(size = 15))
```


```{r}
# Manhattan
fviz_nbclust(x = spotify_perc_50, FUNcluster = pam, method = "wss", diss = dist(spotify_perc_50, method = "manhattan")) +
  labs(title = "Número de clusters óptimos", subtitle = "Distancia Manhattan", x = "Número de clusters", y = "Suma total de los cuadrados (wss) ") +
  theme(plot.title = element_text(size = 20), plot.subtitle = element_text(size = 15))
```

Como podemos ver, con las dos distancias obtenemos el mismo patrón. A partir de estos gráficos podríamos deducir que a partir de $k = 5$ clusters la suma total de los cuadrados internos empieza a estabilizarse, por eso consideraremos este valor de $k$. Para aplicar el método PAM, usaremos la distancia euclidea. Veamos con la función de R `r pam` los medoides:


```{r}
set.seed(2218)

spotify_pam <- pam(x = spotify_perc_50, k = 5, metric = "euclidean")

spotify_pam$id.med
```

Estos son los id de las observaciones representantes de los grupos, veamos los valores que toman estas observaciones.


```{r}
id_medoids = c(18,27,34,36,37)

spotify_perc_50 %>% 
  filter(rownames(.) %in% id_medoids) %>%
  mutate(id = id_medoids) %>% 
  relocate(id, .before = "dance%") %>% 
  show()

```
Veamos las agrupaciones en el plano a partir de las componentes principales, solo considerando dos dimensiones.

```{r}
# Para resaltar los medoides
medoids <- prcomp(spotify_perc_50, scale = TRUE)$x

# Se seleccionan únicamente las proyecciones de las observaciones que son medoids

medoids <- medoids[id_medoids, c("PC1", "PC2")]


medoids <- as.data.frame(medoids)
# Se emplean los mismos nombres que en el objeto ggplot
colnames(medoids) <- c("x", "y")


fviz_cluster(object = spotify_pam, data = spotify_perc_50, ellipse.type = "t", repel = TRUE) +
  theme_bw() +
  labs(title = "Representación con Componentes Principales") +
  theme(legend.position = "none") +
  
  #Medoides 
  geom_point(data = medoids, color = "#EE0606", size = 2)

```






## Método de clustering jerárquico aglomerativo













